---
title       : Unsupervised prediction
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
logo        : bloomberg_shield.png
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow   # 
url:
  lib: ../../libraries
  assets: ../../assets
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
require(knitr)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig_026_unsupervised_prediction/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Key ideas

* Sometimes you don't know the labels for prediction
* To build a predictor
  * Create clusters
  * Name clusters
  * Build predictor for clusters
* In a new data set
  * Predict clusters


---

## Iris example ignoring species labels

```{r libraries, cache=FALSE}
library(ggplot2); library(caret)
```

```{r iris}
data(iris)
inTrain <- createDataPartition(y=iris$Species,
                              p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training); dim(testing)
```

Here we've divided the data into training and test sets like we have in previous lectures, but we're going to ignore the 'Species' clusters.

---

## Cluster with k-means
In Exploratory Data Analysis, we learned about K-means clustering. We'll do that again here, forming 3 clusters while deliberately ignoring the Species variable.

Then we'll plot the data, assigning colors to the clusters.

```{r kmeans,dependson="iris",fig.height=4,fig.width=6}
kMeans1 <- kmeans(subset(training,select=-c(Species)),centers=3)
training$clusters <- as.factor(kMeans1$cluster)
qplot(Petal.Width,Petal.Length,colour=clusters,data=training)
```


---

## Compare to real labels
We can see that our clusters correspond well to the Species variable.

```{r ,dependson="kmeans"}
table(kMeans1$cluster,training$Species)
```

The idea is that we wouldn't know that the clusters were species, and so we'd just name them something.

---

## Build predictor

We'll fit a model on those clusters with the training set, and using a classification method.

```{r modelFit,dependson="kmeans"}
modFit <- train(clusters ~.,data=subset(training,select=-c(Species)),method="rpart")
table(predict(modFit,training),training$Species)
```

The prediction on the training set wasn't so hot. That's because there is both error and variation in the prediction building, as well as in the cluster building.
And that is why unsupervised learning is quite challenging.

---

## Apply on test

```{r ,dependson="modFit"}
testClusterPred <- predict(modFit,testing) 
table(testClusterPred ,testing$Species)
```

---

## Notes and further reading

* The cl_predict function in the clue package provides similar functionality
    * But it often makes sense to build your own because you really need to think carefully about how to define the clusters.
* Beware over-interpretation of clusters!
    * The clusters may change depending on how you've sampled the data. 
* This is one basic approach to [recommendation engines](http://en.wikipedia.org/wiki/Recommender_system)
* [Elements of statistical learning](http://www-stat.stanford.edu/~tibs/ElemStatLearn/)
* [Introduction to statistical learning](http://www-bcf.usc.edu/~gareth/ISL/)
