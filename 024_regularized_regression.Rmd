---
title       : Regularized regression
subtitle    : 
author      : Jeffrey Leek
job         : Johns Hopkins Bloomberg School of Public Health
logo        : bloomberg_shield.png
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow   # 
url:
  lib: ../../librariesNew
  assets: ../../assets
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
require(knitr)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, cache=TRUE,tidy = F, cache.path = '.cache/', fig.path = 'fig_024_regularized_regr/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```


## Basic idea

1. Fit a regression model
2. Penalize (or shrink) large coefficients

__Pros:__

* Can help with the bias/variance tradeoff
    * If two variables are highly correlated, we would like to eliminate one to reduce the variance. This will increase bias a little, but decrease variance a lot.
* Can help with model selection

__Cons:__

* May be computationally demanding on large data sets
* Does not perform as well as random forests and boosting



---

## A motivating example

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon$$

where $X_1$ and $X_2$ are nearly perfectly correlated (co-linear). You can approximate this model by:

$$Y = \beta_0 + (\beta_1 + \beta_2)X_1 + \epsilon$$

The result is:

* You will get a good estimate of $Y$
* The estimate (of $Y$) will be biased 
* We may reduce variance in the estimate

---

## Prostate cancer 

```{r prostate}
library(ElemStatLearn); data(prostate)
str(prostate)
```



---

## Subset selection
Here we see residuals plot which compares the training and test sets over an increasing number of predictors.

Notice that as we include more predictors, the test data becomes more accurate to a point, plateaus, and then becomes less accurate.

This is because the plateau is the border beyond which we are overfitting the model.

The error rate of the training set, naturally, goes monotonically down as more predictors are included.

<img class="center" src="https://raw.githubusercontent.com/DataScienceSpecialization/courses/master/assets/img/prostate.png" height="450">

[Code here](http://www.cbcb.umd.edu/~hcorrada/PracticalML/src/selection.R)


---

## Most common pattern

<img class="center" src="https://raw.githubusercontent.com/DataScienceSpecialization/courses/master/assets/img/trainingandtest.png" height="450">

http://www.biostat.jhsph.edu/~ririzarr/Teaching/649/

---

## Model selection approach: split samples
By using a validation set, we can try to find the minimum error / maximum number of predictors.

* No method better when data/computation time permits it

* Approach
  1. Divide data into training/test/validation
  2. Treat validation as test data, train all competing models on the train data and pick the best one on validation. 
  3. To appropriately assess performance on new data apply to test set
  4. You may re-split and reperform steps 1-3

* Two common problems
  * Limited data - It's difficult to get a good model fit if the data set is small.
  * Computational complexity - Trying all possible subset of models with many predictor variables is time consuming.
  
http://www.biostat.jhsph.edu/~ririzarr/Teaching/649/
http://www.cbcb.umd.edu/~hcorrada/PracticalML/


---

## Decomposing expected prediction error
If we assume that the variable $Y$ can be predicted as a function of $X$ plus an error term, then the expected predicton error ($EPE$) is the expected difference between the outcome ($Y$) and the prediction of the outcome ($\hat{f}_{\lambda}(X)$), quantity squared.

Assume $Y_i = f(X_i) + \epsilon_i$

$EPE(\lambda) = E\left[\{Y - \hat{f}_{\lambda}(X)\}^2\right]$

Suppose $\hat{f}_{\lambda}$ is the estimate from the training data and look at a new data point $X = x^*$:
$$E\left[\{Y - \hat{f}_{\lambda}(x^*)\}^2\right]$$

That equation can be cleverly decomposed into:

<center> = Irreducible error + Bias$^2$ + Variance </center>

$$ = \sigma^2 + \{E[\hat{f}_{\lambda}(x^*)] - f(x^*)\}^2 + var[\hat{f}_\lambda(x_0)]$$

*The goal of building any prediction model is to reduce the expected mean squared error, which consists of Irreducible error + Bias$^2$ + Variance, and of which only bias and variance can be reduced.*

http://www.biostat.jhsph.edu/~ririzarr/Teaching/649/
http://www.cbcb.umd.edu/~hcorrada/PracticalML/

---

## Another issue for high-dimensional data
When you have more predictor variables than data observations (in this case, 5), not all the coefficients will be able to be calculated, resulting in NAs, and a non-invertible design matrix. (I think there can only be at most as many coefficients as there are observations.)

```{r ,dependson="prostate"}
small = prostate[1:5,]
lm(lpsa ~ .,data =small)
```

http://www.biostat.jhsph.edu/~ririzarr/Teaching/649/
http://www.cbcb.umd.edu/~hcorrada/PracticalML/

---

## Hard thresholding
One way of dealing with the problem of too many predictors / not enough data is to: 

* Assume a Model: $Y = f(X) + \epsilon$

* Assume the model is linear: $\hat{f}_{\lambda}(x) = x'\beta$

* Constrain only $\lambda$ coefficients to be nonzero. 

* Try all possible combination of $\lambda$ coefficients. A selection problem is that after chosing $\lambda$, we have to figure out which $p - \lambda$ coefficients to make nonzero, and compare all models to find the best one.

* That approach is computationally expensive.

http://www.biostat.jhsph.edu/~ririzarr/Teaching/649/
http://www.cbcb.umd.edu/~hcorrada/PracticalML/

---

## Regularization for regression
Here's a better approach than Hard Thresholding.

If the coefficients $\beta_j$'s are unconstrained, that is, we don't assume a linear model (or any other):
* Highly correlated variables can explode,
* And hence are susceptible to very high variance

To control variance, we might regularize/shrink the coefficients. 

$$ PRSS(\beta) = \sum_{j=1}^n (Y_j - \sum_{i=1}^m \beta_{1i} X_{ij})^2 + P(\lambda; \beta)$$

* where again, $Y$ is the outcome, and $\beta X$ is the linear model fit.
* where $PRSS$ is a penalized form of the sum of squares. 
* $P$ is the Penalty, which shrinks the coefficients that are too big.

Things that are commonly looked for

* Penalty reduces complexity
* Penalty reduces variance
* Penalty respects structure of the problem, if you set it up in the right way.

---

## Ridge regression

Solve:

$$ \sum_{i=1}^N \left(y_i - \beta_0 + \sum_{j=1}^p x_{ij}\beta_j \right)^2 + \lambda \sum_{j=1}^p \beta_j^2$$

In this form, the penalty term requires that some of the Betas are small. If the sum of all the \beta_j^2 terms is big then the penalty term will be too big, and we won't get a good fit.

equivalent to solving

$\sum_{i=1}^N \left(y_i - \beta_0 + \sum_{j=1}^p x_{ij}\beta_j \right)^2$ subject to $\sum_{j=1}^p \beta_j^2 \leq s$ where $s$ is inversely proportional to $\lambda$ 


Inclusion of $\lambda$ makes the problem non-singular even if $X^TX$ is not invertible.

http://www.biostat.jhsph.edu/~ririzarr/Teaching/649/
http://www.cbcb.umd.edu/~hcorrada/PracticalML/


---

## Ridge coefficient paths
For every different choice of $\lambda$ in the penalized regression, as $\lambda$ increases, we penalize the big betas more and more. 

<img class="center" src="https://raw.githubusercontent.com/DataScienceSpecialization/courses/master/assets/img/ridgepath.png" height="450">

You can see that compared to having no penalty term, a higher $\lambda$ forces all the coefficients to be closer to zero.

http://www.biostat.jhsph.edu/~ririzarr/Teaching/649/
http://www.cbcb.umd.edu/~hcorrada/PracticalML/

---

## Tuning parameter $\lambda$

* $\lambda$ controls the size of the coefficients
* $\lambda$ controls the amount of {\bf regularization}
* As $\lambda \rightarrow 0$ we obtain the least square solution
* As $\lambda \rightarrow \infty$ we have $\hat{\beta}_{\lambda=\infty}^{ridge} = 0$

Picking the tuning parameter can be done with cross validation or other techniques, looking for the optimal trade-off between bias and variance.

---

## Lasso 

The Lasso method shinks all the coefficients and sets some of them to zero, which is like performing model selection.

Here again we have a least squares problem in which we want to find the best combination of beta values that result in the smallest distance to the outcome.

$\sum_{i=1}^N \left(y_i - \beta_0 + \sum_{j=1}^p x_{ij}\beta_j \right)^2$

We can constrain a subject to the sum of the absolute values of only the beta_j terms that are less than some value $s$:

$\sum_{j=1}^p |\beta_j| \leq s$ 

This can also be written as a penalized regression in a lagrangian form: 

$$ \sum_{i=1}^N \left(y_i - \beta_0 + \sum_{j=1}^p x_{ij}\beta_j \right)^2 + \lambda \sum_{j=1}^p |\beta_j|$$

For orthonormal design matrices (not the norm!) this has a closed form solution, in which we take the absolute value of $\hat{\beta}_j^0$ and subtract off a $\gamma$ value, and take only the positive part. So if $\gamma$ is greater than $\hat{\beta}_j^0$ then we'll ignore that coefficient. But if $\gamma$ is smaller than it will shrink that coefficient. Then we give back the original sign of $\hat{\beta}_j^0$.

$$\hat{\beta}_j = sign(\hat{\beta}_j^0)(|\hat{\beta}_j^0| - \gamma)^{+}$$
 
but not in general. <---(Not sure what that he means by this.)

http://www.biostat.jhsph.edu/~ririzarr/Teaching/649/
http://www.cbcb.umd.edu/~hcorrada/PracticalML/


---

## Notes and further reading


* [Hector Corrada Bravo's Practical Machine Learning lecture notes](http://www.cbcb.umd.edu/~hcorrada/PracticalML/)
* [Hector's penalized regression reading list](http://www.cbcb.umd.edu/~hcorrada/AMSC689.html#readings)
* [Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/)
* In `caret` methods are:
  * `ridge`
  * `lasso`
  * `relaxo`
