---
title       : Preprocessing
subtitle    : 
author      : Jeffrey Leek
job         : Johns Hopkins Bloomberg School of Public Health
logo        : bloomberg_shield.png
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow   # 
url:
  lib: ../../libraries
  assets: ../../assets
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
require(knitr)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig_014_preprocess/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```


## Why preprocess?

```{r libraries, cache=FALSE}
library(caret); library(kernlab); library(RANN)
```
```{r loadPackage,cache=TRUE,fig.height=3.5,fig.width=3.5}
data(spam) ## from kernlab
inTrain <- createDataPartition(y=spam$type,
                              p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
hist(training$capitalAve,main="",xlab="ave. capital run length")
```
Here we've made a histogram of the frequency of emails with different run lengths of capital letters, i.e. "THIS EMAIL IS ALL CAPS SO BUY SOMETHING NOW." 

Nearly all of the emails have short run lengths, but there are a few that are much longer. This is an example of a variable that is very skewed, which makes it difficult to deal with in model based predictors, so we'll preprocess.

---

## Why preprocess?
The standard deviation here is much much larger than the mean.

```{r ,dependson="loadPackage",cache=TRUE,fig.height=3.5,fig.width=3.5}
mean(training$capitalAve)
sd(training$capitalAve)
```

---

## Standardizing

Standardizing will turn the mean into 0 and the sd into 1.
```{r ,dependson="loadPackage",cache=TRUE,fig.height=3.5,fig.width=3.5}
trainCapAve <- training$capitalAve
trainCapAveS <- (trainCapAve  - mean(trainCapAve))/sd(trainCapAve) 
mean(trainCapAveS)
sd(trainCapAveS)
```

---

## Standardizing - test set
To standardize the test set, we **must** use the mean and sd from the training set.

```{r ,dependson="loadPackage",cache=TRUE,fig.height=3.5,fig.width=3.5}
testCapAve <- testing$capitalAve
testCapAveS <- (testCapAve  - mean(trainCapAve))/sd(trainCapAve) 
mean(testCapAveS)
sd(testCapAveS)
```
Hopefully the standardized test mean and sd will be close to 0 and 1, as they are here.

---

## Standardizing - _preProcess_ function
The caret package has a `preProcess()` function which can easily standardize the data. We'll create an object (we'll call `preObj`) which contains the parameters we want to use to preprocess the data. We pass the parameter `method` the functions "center" and "scale". And then we can apply that to our training set with `predict()`.

```{r preprocess,dependson="loadPackage",cache=TRUE,fig.height=3.5,fig.width=3.5}
preObj <- preProcess(training[,-58],method=c("center","scale"))  ## column 58 is the outcome variable, so we eliminate that one.
trainCapAveS <- predict(preObj,training[,-58])$capitalAve
mean(trainCapAveS)
sd(trainCapAveS)
```

---

## Standardizing - _preProcess_ function
We can also use `preObj` to apply that same preprocessing to the test set.   
```{r ,dependson="preprocess",cache=TRUE,fig.height=3.5,fig.width=3.5}
testCapAveS <- predict(preObj,testing[,-58])$capitalAve
mean(testCapAveS)
sd(testCapAveS)
```

---

## Standardizing - _preProcess_ argument
You can also pass the preProcess commands directly to the `train()` function as an argument.

```{r training, dependson="loadPackage",cache=TRUE}
set.seed(32343)
modelFit <- train(type ~.,data=training,
                  preProcess=c("center","scale"),method="glm")
modelFit
```


---

## Standardizing - Box-Cox transforms
The Box-Cox Transforms are a set of transformations that take continuous data and try to make them look like Normal data by estimating a specific set of parameters using maximum likelihood.

Here we use the "BoxCox" method in the `preProcess()` function.

```{r ,dependson="loadPackage",cache=TRUE,fig.height=3.5,fig.width=7}
preObj <- preProcess(training[,-58],method=c("BoxCox"))
trainCapAveS <- predict(preObj,training[,-58])$capitalAve
par(mfrow=c(1,2)); hist(trainCapAveS); qqnorm(trainCapAveS)
```
In this case, we can see that the data is somewhat normal but that there are a bunch of data points down near zero, that it can't do anything about. It does help a lot with data that are highly skewed.

---

## Standardizing - Imputing data
Prediction algorithms often fail with missing data. So we like to impute values for the missing data.

Here we impute them with "K nearest neighbors imputation" which is `method='knnImpute'.  

We'll set the seed here because this is a randomized algorithm and we want to get reproducible results, and we'll first make some NA values because this data set doesn't have any.

```{r knn,dependson="loadPackage",cache=TRUE,fig.height=3.5,fig.width=7}
set.seed(13343)

# Make some values NA
training$capAve <- training$capitalAve
selectNA <- rbinom(dim(training)[1],size=1,prob=0.05)==1
training$capAve[selectNA] <- NA

# Impute and standardize
preObj <- preProcess(training[,-58],method="knnImpute")
capAve <- predict(preObj,training[,-58])$capAve

# Standardize true values
capAveTruth <- training$capitalAve
capAveTruth <- (capAveTruth-mean(capAveTruth))/sd(capAveTruth)
```


---

## Standardizing - Imputing data
Because we created the NAs, we had actual values which we can now compare to our imputed values.

We'll do that by subracting one group from the other and looking at the quantiles. Here we have the quantiles of the all the data, followed by just the imputed values (from the variable `selectNA`), followed by just the values that weren't imputed.
```{r ,dependson="knn",cache=TRUE,fig.height=3.5,fig.width=7}
quantile(capAve - capAveTruth)
quantile((capAve - capAveTruth)[selectNA])
quantile((capAve - capAveTruth)[!selectNA])
```
The numbers close to zero indicate that our imputations are pretty good.

---

## Notes and further reading

* Training and test must be processed in the same way
* Test transformations will likely be imperfect
  * Especially if the test/training sets collected at different times
* Careful when transforming factor variables!
* [preprocessing with caret](http://caret.r-forge.r-project.org/preprocess.html)