---
title       : Boosting
subtitle    : 
author      : Jeffrey Leek
job         : Johns Hopkins Bloomberg School of Public Health
logo        : bloomberg_shield.png
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow   # 
url:
  lib: ../../librariesNew
  assets: ../../assets
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
require(knitr)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, cache=TRUE,tidy = F, cache.path = '.cache/', fig.path = 'fig_022_boosting/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```


## Basic idea

1. Take lots of (possibly) weak predictors.
2. Weight them and add them up.
3. Average them and get a stronger predictor.


---

## Basic idea behind boosting

1. Start with a set of classifiers $h_1,\ldots,h_k$
  * Examples: All possible trees, all possible regression models, all possible cutoffs.
2. Create a classifier that combines classification functions:
$f(x) = \rm{sgn}\left(\sum_{t=1}^T \alpha_t h_t(x)\right)$.

* where:
    * $\alpha_t$ is a weight,
    * $h_t(x)$ is a classifier,
    * $f(x)$ is the prediction for a new point.
  
* Goal is to minimize error (on training set)
    * Iterative, select one $h$ at each step,
    * calculate weights for the next step, based on errors from the previous $h$,
    * then upweight missed classifications and select next $h$.
  
The most famous boosting algorithm is Adaboost.
[http://en.wikipedia.org/wiki/AdaBoost](http://en.wikipedia.org/wiki/AdaBoost)

[http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf](http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf)

---

## Simple example
Suppose we're trying to separate the blue + signs from the red - signs, and we have two variables to predict with. Here we've plotted variable 1 on the x-axis and variable 2 on the y-axis. We'll make a very simple classifier which simply draws a straight orthogonal line.

<img class=center src="https://raw.githubusercontent.com/DataScienceSpecialization/courses/master/assets/img/08_PredictionAndMachineLearning/ada1.png" height=450>

[http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf](http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf)

---

## Round 1: adaboost
First we'll define a vertical line for which anything to its left is a blue + and anything to its right is a red -.

<img class=center src="https://raw.githubusercontent.com/DataScienceSpecialization/courses/master/assets/img/08_PredictionAndMachineLearning/adar1.png" height=450>

You can see that we've misclassified three points at the top right. We calculate the error rate which says we're missing 30% of the points, and we'll upweight the missed points.

[http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf](http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf)

---

## Round 2 & 3
Then in round 2, we build the next classifier using those weights, and this classifier draws a new vertical line that ensures those weighted points are on its left.

Now there are 3 new points (red -) that are misclassified. The error rate is calculated and used to upweight hose 3 points for the next round.
 
<img class=center src="https://raw.githubusercontent.com/DataScienceSpecialization/courses/master/assets/img/08_PredictionAndMachineLearning/ada2.png" height=450>

In round 3, a horizontal line is drawn to ensure all the weighted points are separated. Again there are misclassified points, and we calculate an error rate, and upweight them. But we've reached our last round. 

[http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf](http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf)


---

## Completed classifier
Now we classify a weighted combination which sums our 3 weighted classifiers. The combined classifier draws a more complicated line which correctly classifies all the points.  

<img class=center src="https://raw.githubusercontent.com/DataScienceSpecialization/courses/master/assets/img/08_PredictionAndMachineLearning/ada3.png" height=450>

[http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf](http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf)

---

## Boosting in R 

* Boosting can be used with any subset of classifiers
* One large subclass is [gradient boosting](http://en.wikipedia.org/wiki/Gradient_boosting)
* R has multiple boosting libraries. Differences include the choice of basic classification functions and combination rules.
  * [gbm](http://cran.r-project.org/web/packages/gbm/index.html) - boosting with trees.
  * [mboost](http://cran.r-project.org/web/packages/mboost/index.html) - model based boosting
  * [ada](http://cran.r-project.org/web/packages/ada/index.html) - statistical boosting based on [additive logistic regression](http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1016218223)
  * [gamBoost](http://cran.r-project.org/web/packages/GAMBoost/index.html) for boosting generalized additive models
* Most of these are available in the caret package 



---

## Wage example
```{r libraries, cache=FALSE}
library(ISLR); library(ggplot2); library(caret);
```
```{r wage, cache=TRUE}
data(Wage)
Wage <- subset(Wage,select=-c(logwage))
inTrain <- createDataPartition(y=Wage$wage,
                              p=0.7, list=FALSE)
training <- Wage[inTrain,]; testing <- Wage[-inTrain,]
```


---

## Fit the model
Here we'll use `method="gbm"` which does boosting with trees.

```{r, dependson="wage", cache=TRUE}
modFit <- train(wage ~ ., method="gbm",data=training,verbose=FALSE)
print(modFit)
```

When we print the model fit, you can see that there are different numbers of trees that are used, and different interaction depths, which are used together to build a boosted version of regression trees.

---

## Plot the results
Here we plot the predicted results from the test set using our model fit versus the outcome wage from the test set. 

```{r, dependson="wage", fig.height=4,fig.width=4}
qplot(predict(modFit,testing),wage,data=testing)
```


It's a reasonably good prediction, though there is still a lot of variability.

The basic idea for fitting a boosted algorithm is to take these weak classifiers and average them together with weights in order to get a better classifier.

---

## Notes and further reading

* A couple of nice tutorials for boosting
  * Freund and Shapire - [http://www.cc.gatech.edu/~thad/6601-gradAI-fall2013/boosting.pdf](http://www.cc.gatech.edu/~thad/6601-gradAI-fall2013/boosting.pdf)
  * Ron Meir- [http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf](http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf)
* Boosting, random forests, and model ensembling are the most common tools that win Kaggle and other prediction contests. 
  * [http://www.netflixprize.com/assets/GrandPrize2009_BPC_BigChaos.pdf](http://www.netflixprize.com/assets/GrandPrize2009_BPC_BigChaos.pdf)
  * [https://kaggle2.blob.core.windows.net/wiki-files/327/09ccf652-8c1c-4a3d-b979-ce2369c985e4/Willem%20Mestrom%20-%20Milestone%201%20Description%20V2%202.pdf](https://kaggle2.blob.core.windows.net/wiki-files/327/09ccf652-8c1c-4a3d-b979-ce2369c985e4/Willem%20Mestrom%20-%20Milestone%201%20Description%20V2%202.pdf)
  